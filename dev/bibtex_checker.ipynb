{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WreoZMPfY-I8"
   },
   "outputs": [],
   "source": [
    "from unidecode import unidecode as decode\n",
    "import bibtexparser as bp\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from urllib import request as get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nGkLxVvarNcb"
   },
   "outputs": [],
   "source": [
    "def remove_accents_and_hyphens(s):\n",
    "  replace = {'{\\\\l}': 'l',\n",
    "             '{\\\\o}': 'o',\n",
    "             '{\\\\i}': 'i',\n",
    "             '{\\\\t}': 't'}\n",
    "    \n",
    "  for key, val in replace.items():\n",
    "    s = s.replace(key, val)\n",
    "  \n",
    "  accents = r'''\\{?\\\\[`'^\"~=.uvHtcdbkr]?\\s?\\{?\\\\?(\\w*)\\}?'''\n",
    "  accented_chars = [x for x in re.finditer(accents, s)]\n",
    "\n",
    "  s_list = [c for c in s]\n",
    "  hyphen = '-'\n",
    "  for a in accented_chars:\n",
    "    next_len = a.end() - a.start()\n",
    "    s_list[a.start()] = a.group(1)\n",
    "    s_list[(a.start() + 1):a.end()] = hyphen * (next_len - 1)\n",
    "  return ''.join([c for c in s_list if c != hyphen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1lifLiOylvbG"
   },
   "outputs": [],
   "source": [
    "def match(names, template):\n",
    "  #check if list of strings names contains a match to the potentially multi-word\n",
    "  #template.  return the position of the start of the left-most match\n",
    "  template = template.split(' ')\n",
    "  for i, x in enumerate(names[:len(names) - len(template) + 1]):\n",
    "    found_match = False\n",
    "    for j, t in enumerate(template):\n",
    "      if names[i + j].lower() != t:\n",
    "        found_match = False\n",
    "        break\n",
    "      else:\n",
    "        found_match = True\n",
    "    if found_match:\n",
    "      return i\n",
    "  return -1\n",
    "\n",
    "prefixes = ['de', 'da', 'di', 'von', 'zu', 'van', 'du', 'des',\n",
    "            'del', 'de la', 'della', 'la', 'le', 'der', 'af',\n",
    "            'st.', 'saint', 'st', 'dom', 'do', 'das', 'dos', 'of', 'al',\n",
    "            'el', 'dei', 'tot', 'thoe', 'aw', 'na', 'sri', 'phra',\n",
    "            'si', 'shri', 'lo', 'no', 'op', 'lopes', 'gonzalez', 'vom', 'castro']\n",
    "\n",
    "suffixes = ['sr.', 'jr.', 'sr', 'jr', 'senior', 'junior', 'iii', 'iv', 'v', \n",
    "            'vi', 'vii', 'viii', 'ix', 'x', 'the', 'third', 'fourth', 'fifth',\n",
    "            'sixth', 'seventh', 'eighth', 'ninth', 'tenth', 'great']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZLQJyV_NKW0w"
   },
   "outputs": [],
   "source": [
    "def remove_curlies(s): #only removes *matching* curly braces\n",
    "  for i, c in enumerate(s):    \n",
    "    if c == '{':\n",
    "      j = i\n",
    "      \n",
    "      curly_count = 1\n",
    "      while (j < len(s) - 1):\n",
    "        j += 1\n",
    "        if s[j] == '{':\n",
    "          curly_count += 1\n",
    "\n",
    "        if s[j] == '}':\n",
    "          curly_count -= 1\n",
    "        \n",
    "        if curly_count == 0:\n",
    "          break\n",
    "      \n",
    "      if curly_count == 0:\n",
    "        return remove_curlies(s[:i] + ''.join(s[(i+1):j].split(' ')) + s[(j+1):])\n",
    "\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "a5Alp9RDzIsU"
   },
   "outputs": [],
   "source": [
    "def remove_non_letters(s):\n",
    "  remove_chars = [',', '.', '!', '?', \"'\", '\"', '{', '}', '-']\n",
    "  \n",
    "  for c in remove_chars:\n",
    "    s = s.replace(c, '')\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qE7Ch9hhrqgL"
   },
   "outputs": [],
   "source": [
    "def rearrange(name):\n",
    "  original_name = name\n",
    "\n",
    "  #remove suffixes and convert to list\n",
    "  name = [remove_non_letters(x.strip()) for x in name.split(',')]\n",
    "  sxs = [n for n in name if n.lower() in suffixes]\n",
    "  name = [n for n in name if n.lower() not in suffixes]\n",
    "  \n",
    "  if len(name) == 2: #last, first (+ middle)\n",
    "    x = ' '.join([name[1], name[0]])\n",
    "  elif len(name) == 1: #first (+ middle) + last\n",
    "    x = name[0]\n",
    "  elif len(name) == 0:\n",
    "    raise Exception(f'no non-suffix names: {original_name}')\n",
    "  elif len(name) > 2:\n",
    "    raise Exception(f'too many commas: {original_name}')\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MmJCn3oROIu6"
   },
   "outputs": [],
   "source": [
    "def last_name(names):\n",
    "  #remove suffixes and non-letters\n",
    "  names = [remove_non_letters(n) for n in rearrange(names).split(' ') if not (n.lower() in suffixes)]\n",
    "\n",
    "  #start at the end and move backward\n",
    "  x = []\n",
    "  found_prefix = False\n",
    "  for n in reversed(names):\n",
    "    if n.lower() in prefixes:\n",
    "      found_prefix = True\n",
    "    elif found_prefix or len(x) > 0:\n",
    "      break\n",
    "    x.append(n)\n",
    "  if found_prefix:\n",
    "    return ''.join(reversed(x))\n",
    "  else:\n",
    "    return x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "24P8HXnYwp2R"
   },
   "outputs": [],
   "source": [
    "def authors2key(authors, year):\n",
    "  def key(author):\n",
    "    #convert accented unicode characters to closest ascii equivalent\n",
    "    author = decode(author)\n",
    "\n",
    "    #re-arrange author name to FIRST [MIDDLE] LAST [SUFFIX]\n",
    "    author = remove_accents_and_hyphens(author)\n",
    "    author = remove_curlies(author)\n",
    "\n",
    "    #get first 4 letters of last name\n",
    "    return last_name(author)[:4]\n",
    "  \n",
    "  yr_str = str(year)[-2:]\n",
    "\n",
    "  authors = authors.split(' and ')\n",
    "  if len(authors) == 0:\n",
    "    raise Exception('Author information missing, no key generated')\n",
    "  elif len(authors) == 1:\n",
    "    return key(authors[0]) + yr_str\n",
    "  elif len(authors) == 2:\n",
    "    return key(authors[0]) + key(authors[1]) + yr_str\n",
    "  elif len(authors) >= 3:\n",
    "    return key(authors[0]) + 'Etal' + yr_str\n",
    "  else:\n",
    "    raise Exception('Something went wrong...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Cf4F1qu8xsan"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type webpage not standard. Not considered.\n",
      "Entry type webpage not standard. Not considered.\n"
     ]
    }
   ],
   "source": [
    "bibfile = '../memlab.bib'\n",
    "parser = bp.bparser.BibTexParser(ignore_nonstandard_types=True, common_strings=True, homogenize_fields=True)\n",
    "\n",
    "with open(bibfile, 'r') as b:\n",
    "    bibdata = bp.load(b, parser=parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "E7_JykrNejFQ"
   },
   "outputs": [],
   "source": [
    "bd = bibdata.get_entry_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "l6WZ6axmhiwq"
   },
   "outputs": [],
   "source": [
    "#get all fields\n",
    "fields = {}\n",
    "for k in bd.keys():\n",
    "  next_entry = bd[k]\n",
    "  for field, vals in next_entry.items():\n",
    "    if not (field in fields.keys()):\n",
    "      fields[field] = [vals]\n",
    "    else:\n",
    "      fields[field].append(vals)\n",
    "\n",
    "for k in fields.keys():\n",
    "  fields[k] = list(np.unique(fields[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "GFlECp9k45wX"
   },
   "outputs": [],
   "source": [
    "def get_vals(bd, field):\n",
    "  def safe_get(item, field):\n",
    "    if field in item.keys():\n",
    "      return item[field]\n",
    "    else:\n",
    "      return ''\n",
    "  \n",
    "  return [safe_get(i, field) for k, i in bd.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "DNpqFsebUQLa"
   },
   "outputs": [],
   "source": [
    "def same_id(a, b, ignore_special=False):\n",
    "  if ignore_special:\n",
    "    if ('\\\\' in a) or ('\\\\' in b):\n",
    "      return True\n",
    "  \n",
    "  if len(a) > len(b):\n",
    "    return same_id(b, a)\n",
    "  elif a == b:\n",
    "    return True\n",
    "  else:\n",
    "    return a == b[:len(a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "FoJMMQ8kyGwu"
   },
   "outputs": [],
   "source": [
    "authors = get_vals(bd, 'author')\n",
    "years = get_vals(bd, 'year')\n",
    "ids = get_vals(bd, 'ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "BzxiFKxw5Y-N"
   },
   "outputs": [],
   "source": [
    "gen_ids = [authors2key(a, y) for a, y in zip(authors, years)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ucd05y6s5xdu",
    "outputId": "94ca107d-240b-4263-f4a5-689c00b30eeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats!  No keys to fix.\n"
     ]
    }
   ],
   "source": [
    "#check keys\n",
    "tofix = [f'{h}. [{i}] should be [{g}]' for i, g, h in zip(ids, gen_ids, range(0, len(ids))) if not same_id(i, g, ignore_special=False)] #check these over carefully...\n",
    "if len(tofix) == 0:\n",
    "  print('Congrats!  No keys to fix.')\n",
    "else:\n",
    "  print(f'Need to fix {len(tofix)} keys: \\n')\n",
    "  print('\\n'.join(tofix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oFAn0HVAJrB5"
   },
   "outputs": [],
   "source": [
    "#check duplicates: same title + author\n",
    "titles = get_vals(bd, 'title')\n",
    "unique_titles, title_counts = np.unique(titles, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "svuDqAXUK_L-"
   },
   "outputs": [],
   "source": [
    "ignore_authors = []\n",
    "\n",
    "last_names = [' and '.join([last_name(a) for a in authors[i].split(' and ') if not (a in ignore_authors)]) for i in range(len(authors))]\n",
    "duplicates = []\n",
    "for t in [t for i, t in enumerate(unique_titles) if title_counts[i] > 1]:\n",
    "  inds = [i for i, x in enumerate(titles) if x == t]\n",
    "  author_names, author_counts = np.unique([last_names[i] for i in inds], return_counts=True)\n",
    "  for a in [a for i, a in enumerate(author_names) if author_counts[i] > 1]:\n",
    "    duplicates.append([i for i, x in enumerate(last_names) if x == a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zzkYatOKQHRl",
    "outputId": "6b14f1b5-5f44-497d-927f-21aa9bd52aed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "vto6mvqTNL7W"
   },
   "outputs": [],
   "source": [
    "for d in duplicates:\n",
    "  print(f'Duplicate IDs: {[[i, ids[i]] for i in d]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Uz7-6pRYiGgH"
   },
   "outputs": [],
   "source": [
    "remove_fields = ['date-modified', 'date-added',\n",
    "                 'note', 'bdsk-url-1', 'pst', 'pmid', 'pmc',\n",
    "                 'mesh', 'keyword', 'journal-full', 'abstract',\n",
    "                 'mendeley-groups', 'file', 'bdsk-url-2', 'eprint',\n",
    "                 'arxivid', 'archiveprefix', 'ty', 'm3', 'l3',\n",
    "                 'howpublished', 'lccn', 'read', 'annote', 'owner',\n",
    "                 'timestamp', 'pii', 'zb', 'z9', 'z8' ,'times-cited',\n",
    "                 'publication-type', 'isi', 'language', 'stat', 'so',\n",
    "                 'sb', 'rf', 'pubm', 'pt', 'pl', 'phst', 'own', 'mhda',\n",
    "                 'jid', 'ip', 'edat', 'dcom', 'da', 'au', 'aid', 'affiliation',\n",
    "                 'lr', 'gr', 'jt', 'local-url', 'dep', 'mh', 'cin', 'ci',\n",
    "                 'comment', 'con', 'card', 'oto', 'ot', 'unique-id',\n",
    "                 'subject-category', 'number-of-cited-references', 'rating',\n",
    "                 'rn', 'oid', 'issn', 'isbn', 'doc-delivery-number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "xN60kz1wh8EP"
   },
   "outputs": [],
   "source": [
    "keep_fields = [k for k in fields.keys() if k not in remove_fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bw-MKdv7oc4i",
    "outputId": "adebd73b-0c01-476f-9c94-c0e1def59b0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year',\n",
       " 'volume',\n",
       " 'title',\n",
       " 'pages',\n",
       " 'number',\n",
       " 'journal',\n",
       " 'author',\n",
       " 'ENTRYTYPE',\n",
       " 'ID',\n",
       " 'booktitle',\n",
       " 'publisher',\n",
       " 'editor',\n",
       " 'school',\n",
       " 'chapter',\n",
       " 'address',\n",
       " 'month',\n",
       " 'organization',\n",
       " 'doi',\n",
       " 'url',\n",
       " 'date',\n",
       " 'series',\n",
       " 'bdsk-file-1',\n",
       " 'type',\n",
       " 'institution',\n",
       " 'edition',\n",
       " 'location']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNq_ZR0l7c2y"
   },
   "source": [
    "To do:\n",
    "- handle first-author surnames that start with lowercase characters\n",
    "- better handling of curly braces to preserve name ordering\n",
    "- better handling of escape characters\n",
    "- ensure bibtex file compiles and gets parsed correctly\n",
    "- Check duplicate titles/authors (fail if detected)\n",
    "- Check bibtex keys.  Duplicates should be assigned a suffix of 'a', 'b', etc.\n",
    "- If keys match aside from suffix then still allow the bibtex file to \"pass\" as long as all \"matching\" keys are unique *and* all have suffixes *and* the suffixes span a, b, c, ..., etc. without gaps\n",
    "- Correct page numberings:\n",
    "  - Change hyphens to n-dashes\n",
    "  - Change m-dashes to n-dashes\n",
    "  - Remove spaces in page ranges\n",
    "  - Print a warning for strangely formatted pages: leading 0, non-digits, very large numbers (greater than 10K?) but allow bibtex file to pass\n",
    "- remove everything except for \"keep_fields\"\n",
    "  - Also add a \"force\" field that, when present, causes the checker to automatically pass that entry (if set to True); this will be used as a workaround for special cases not handled by the parser/checker\n",
    "- change all urls, dois, and page numbers to lowercase\n",
    "\n",
    "Clean up:\n",
    "- correct all \"strings\" to full journal names\n",
    "- correct all journal abbreviations to full journal names\n",
    "- Ensure no periods in journal names\n",
    "- Ensure no periods at the ends of affiliations or titles\n",
    "- Check for compressed initials (AA --> A A; A.A. --> A A; etc.)\n",
    "\n",
    "Bonus:\n",
    "- Use scholarly to verify information.  However, this is currently unreliable (server seems to hang frequently) and too slow to be viable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "G9N9JD9Wk3kM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "bibtex checker.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
